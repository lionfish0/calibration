{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#uncomment to us the CPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import numpy as np\n",
    "from calibration import CalibrationSystem, SparseModel\n",
    "from calibration.categorical import AltCalibrationSystem\n",
    "import gpflow\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "from calibration import synthetic\n",
    "%matplotlib inline\n",
    "from calibration.categorical import compute_posterior_probs, print_confusion_matrices\n",
    "from calibration.baseline_categorical_methods import collaborative_filtering_SVD,evaluate,get_trust_weights,printresults\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing calibration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "N = 300\n",
    "priorp = np.array([0.333,0.333,0.334])\n",
    "truebees = np.random.choice(3,N,p=priorp)\n",
    "times = []\n",
    "#when people saw the bee\n",
    "#times.append((np.linspace(0,1,N)+np.random.rand(N))/2)\n",
    "#times.append((np.linspace(0,1,N)+np.random.rand(N))/2)\n",
    "times.append(np.linspace(0,1,N))\n",
    "times.append(np.linspace(0,1,N))\n",
    "times.append(np.linspace(0,1,N))\n",
    "times.append(np.linspace(0,1,N))\n",
    "#times = [np.round(t) for t in times]\n",
    "\n",
    "#times.append(np.linspace(0,1,10))\n",
    "#times.append(np.linspace(0,1,10))\n",
    "#times.append(np.linspace(0,1,10))\n",
    "\n",
    "conf_matricesA = []\n",
    "conf_matricesA.append(np.array([[1,0,0],[0,1,0],[0,0,1]]))\n",
    "conf_matricesA.append(np.array([[1,1,1],[1,1,1],[1,1,1]]))\n",
    "conf_matricesA.append(np.array([[1,1,0],[1,1,0],[0,0,1]]))\n",
    "conf_matricesA.append(np.eye(3))\n",
    "\n",
    "conf_matricesB = []\n",
    "conf_matricesB.append(np.array([[1,0,0],[0,1,0],[0,0,1]]))\n",
    "conf_matricesB.append(np.array([[1,0,0],[0,1,0],[0,0,1]]))\n",
    "conf_matricesB.append(np.array([[1,0,1],[0,1,0],[1,0,1]]))\n",
    "conf_matricesB.append(np.eye(3))\n",
    "\n",
    "conf_matricesC = []\n",
    "conf_matricesC.append(np.array([[1.1,1,1],[1,1.1,1],[1,1.1,1]]))\n",
    "conf_matricesC.append(np.array([[1,0,0],[0,1,0],[0,0,1]]))\n",
    "conf_matricesC.append(np.array([[1.1,1,1],[1,1.1,1],[1,1.1,1]]))\n",
    "conf_matricesC.append(np.eye(3))\n",
    "\n",
    "def genguess(conf_matrix,truebee):\n",
    "    return np.random.choice(conf_matrix.shape[0],1,p=conf_matrix[truebee,:])\n",
    "\n",
    "D = np.full([len(truebees),len(times)],np.nan)\n",
    "trueconfmatrices = []\n",
    "for person in range(len(times)):\n",
    "    truecms = []\n",
    "    for i,(tb,time) in enumerate(zip(truebees,times[person])):\n",
    "        cm = conf_matricesA[person]*max(2.0*(0.5-time),0) + conf_matricesB[person]*max(1.0-2*np.abs(0.5-time),0) + conf_matricesC[person]*max(2.0*(time-0.5),0)\n",
    "        cm = (cm.T/np.sum(cm,1)).T\n",
    "        fill = False\n",
    "        if np.random.rand()<0.6:\n",
    "            fill = True\n",
    "        #if person == len(times)-1:\n",
    "        #    fill = True\n",
    "        if person == len(times)-2:\n",
    "            if np.all(np.isnan(D[i,:person])):\n",
    "                fill = True\n",
    "        if fill:\n",
    "            D[i,person] = genguess(cm,tb)\n",
    "        truecms.append(cm)\n",
    "    trueconfmatrices.append(truecms)\n",
    "refsensor = np.array([0,0,0,1])\n",
    "\n",
    "X,Y = synthetic.buildXY_from_D(D)\n",
    "#replaces index column with times\n",
    "X = np.c_[[times[p][idx] for p,idx in zip(X[:,1],X[:,0])],[times[p][idx] for p,idx in zip(X[:,2],X[:,0])],X[:,1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "tempD = pickle.load(open('D.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(~np.isnan(tempD),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.isnan(tempD[:,-1])),np.sum(~np.isnan(tempD[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "if False:\n",
    "    pickle.dump(X,open('X.pkl','wb'))\n",
    "    pickle.dump(Y,open('Y.pkl','wb'))\n",
    "    pickle.dump(D,open('D.pkl','wb'))\n",
    "    pickle.dump(priorp,open('priorp.pkl','wb'))\n",
    "    pickle.dump(refsensor,open('refsensor.pkl','wb'))\n",
    "    pickle.dump(truebees,open('truebees.pkl','wb'))\n",
    "    pickle.dump(times,open('times.pkl','wb'))\n",
    "    pickle.dump(trueconfmatrices,open('trueconfmatrices.pkl','wb'))\n",
    "else:\n",
    "    X = pickle.load(open('X.pkl','rb'))\n",
    "    Y = pickle.load(open('Y.pkl','rb'))\n",
    "    D = pickle.load(open('D.pkl','rb'))\n",
    "    priorp = pickle.load(open('priorp.pkl','rb'))\n",
    "    refsensor = pickle.load(open('refsensor.pkl','rb'))\n",
    "    truebees = pickle.load(open('truebees.pkl','rb'))\n",
    "    times = pickle.load(open('times.pkl','rb'))\n",
    "    trueconfmatrices = pickle.load(open('trueconfmatrices.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "Z = np.linspace(0,1,2)[:,None]\n",
    "ks = [gpflow.kernels.RBF(25,0.5)] ##suggest 0.5->0.75?\n",
    "kernelindices = [[0]*len(refsensor)]*(len(priorp)**2)\n",
    "\n",
    "cs = AltCalibrationSystem(X, Y, Z, refsensor, int(len(priorp)**2), ks, kernelindices,lr=0.05,minibatchsize=30,priorp=priorp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_record,samps = cs.run(its=500,samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(elbo_record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allprobs = []\n",
    "for sensor,time in enumerate(times):\n",
    "    probs = []\n",
    "    print(sensor)\n",
    "    for t in time:\n",
    "        print(\".\",end=\"\")\n",
    "        probs.append( compute_posterior_probs(cs,sensors=[sensor],t=t,num_samps=1000)[0] )\n",
    "    allprobs.append(probs)\n",
    "allprobs = np.array(allprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(allprobs,open('synthmodelresult_rbf.pkl','wb'))\n",
    "#allprobs = pickle.load(open('synthmodelresult_rbf.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_confusion_matrices(allprobs,conf_matricesA)\n",
    "colfil_results = collaborative_filtering_SVD(D,len(priorp),refsensor)\n",
    "trust = get_trust_weights(D,len(priorp))\n",
    "results = evaluate(D,truebees,priorp,allprobs,colfil_results,cs.refsensor,trust)\n",
    "pickle.dump(results,open('results_RBF.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printresults(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "for person in [0,1,2]:\n",
    "    plt.figure(figsize=[10,10])\n",
    "    ploti = 0\n",
    "    #68 -> 16 - 84\n",
    "    #95 -> 2.5 - 97.5\n",
    "    perc = [2.5,97.5]#[16,84]\n",
    "    for truebee in range(3):\n",
    "        for predbee in range(3):\n",
    "            ploti+=1\n",
    "            plt.subplot(3,3,ploti)\n",
    "\n",
    "            cibottom = np.percentile(allprobs[person,:,:,:,:],perc[0],1)[:,truebee,predbee]\n",
    "            citop = np.percentile(allprobs[person,:,:,:,:],perc[1],1)[:,truebee,predbee]\n",
    "            plt.fill_between(times[person],cibottom,citop,color='green',alpha=0.3)\n",
    "            plt.plot(times[person],np.median(allprobs[person,:,:,:,:],1)[:,truebee,predbee])\n",
    "            plt.plot(times[person],np.array(trueconfmatrices)[person,:,truebee,predbee])\n",
    "            plt.ylim([0,1])\n",
    "            plt.savefig('synth_person%d.pdf' % person)\n",
    "            ##plt.plot(times[person],np.mean(allprobs[person,:,:,:,:],1)[:,truebee,predbee])\n",
    "            #plt.plot(times[person],np.percentile(allprobs[person,:,:,:,:],2.5,1)[:,truebee,predbee])\n",
    "            #plt.plot(times[person],np.percentile(allprobs[person,:,:,:,:],97.5,1)[:,truebee,predbee])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "Z = np.linspace(0,1,1)[:,None]\n",
    "ks = [gpflow.kernels.Bias(25)] ##suggest 0.5->0.75?\n",
    "kernelindices = [[0]*len(refsensor)]*(len(priorp)**2)\n",
    "\n",
    "cs = AltCalibrationSystem(X, Y, Z, refsensor, int(len(priorp)**2), ks, kernelindices,lr=0.05,minibatchsize=30,priorp=priorp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_record,samps = cs.run(its=500,samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allprobs = []\n",
    "for sensor,time in enumerate(times):\n",
    "    probs = []\n",
    "    print(sensor)\n",
    "    for t in time:\n",
    "        print(\".\",end=\"\")\n",
    "        probs.append( compute_posterior_probs(cs,sensors=[sensor],t=t,num_samps=1000)[0] )\n",
    "    allprobs.append(probs)\n",
    "allprobs = np.array(allprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(allprobs,open('synthmodelresult_bias.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_confusion_matrices(allprobs,conf_matricesA)\n",
    "colfil_results = collaborative_filtering_SVD(D,len(priorp),refsensor)\n",
    "trust = get_trust_weights(D,len(priorp))\n",
    "results = evaluate(D,truebees,priorp,allprobs,colfil_results,cs.refsensor,trust)\n",
    "pickle.dump(results,open('results_bias.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printresults(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion_matrices(allprobs[:,0,:,:,:],conf_matricesB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.array([np.sum(truebees==i) for i in range(3)])\n",
    "ts = ts/np.sum(ts)\n",
    "ts*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.isnan(D[:,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "#from calibration import CalibrationSystem, SparseModel, synthetic\n",
    "#import gpflow\n",
    "#import matplotlib.pyplot as plt\n",
    "#import tensorflow as tf\n",
    "import os\n",
    "#uncomment to us the CPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import numpy as np\n",
    "from calibration import CalibrationSystem, SparseModel\n",
    "from calibration.categorical import AltCalibrationSystem\n",
    "import gpflow\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "from calibration import synthetic\n",
    "%matplotlib inline\n",
    "from calibration.categorical import compute_posterior_probs, print_confusion_matrices\n",
    "from calibration.baseline_categorical_methods import collaborative_filtering_SVD,evaluate,get_trust_weights,printresults\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare = ['Bombus bohemicus', 'Bombus jonellus', 'Bombus sylvestris', 'Bombus muscorum','Bombus campestris', 'Apis mellifera','Bombus monticola']\n",
    "    #rare.extend(['Bombus pratorum','Bombus hypnorum','Bombus hortorum']) #temp to make it quick\n",
    "D, priorp, truebees = synthetic.build_D_from_csv('beelabels.csv',rare)\n",
    "print(\"%d non-DNN labels\" % np.sum(~np.isnan(D[:,:-2])))\n",
    "print(\"%d DNN labels\" % np.sum(~np.isnan(D[:,-2])))\n",
    "print(\"%d DNN labels\" % np.sum(~np.isnan(D[:,:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in range(D.shape[1]-1):\n",
    "    d = D[:,p]\n",
    "    inc = (~np.isnan(d)) & (~np.isnan(D[:,-1]))\n",
    "    t = np.sum(inc)\n",
    "    s = np.sum(d[inc]==D[inc,-1])\n",
    "    print(\"%d: %d/%d = %0.2f%%\" % (p,s,t,100*s/t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "allresults = []\n",
    "for crossval in range(5):\n",
    "    rare = ['Bombus bohemicus', 'Bombus jonellus', 'Bombus sylvestris', 'Bombus muscorum','Bombus campestris', 'Apis mellifera','Bombus monticola']\n",
    "    #rare.extend(['Bombus pratorum','Bombus hypnorum','Bombus hortorum']) #temp to make it quick\n",
    "    D, priorp, truebees = synthetic.build_D_from_csv('beelabels.csv',rare)\n",
    "    \n",
    "    #Cross val split here...\n",
    "    #this is where we want to nan some of the ref values in D.\n",
    "    D[crossval::5,-1]=np.nan\n",
    "\n",
    "    \n",
    "    refsensor = np.zeros(D.shape[1])\n",
    "    refsensor[-1] = 1\n",
    "    X,Y = synthetic.buildXY_from_D(D)\n",
    "\n",
    "    Z = np.linspace(0,1,1)[:,None]\n",
    "    ks = [gpflow.kernels.Bias(100)]\n",
    "    kernelindices = [[0]*len(refsensor)]*(len(priorp)**2)\n",
    "\n",
    "    cs = AltCalibrationSystem(X, Y, Z, refsensor, int(len(priorp)**2), ks, kernelindices,lr=0.04,minibatchsize=10,priorp=priorp)\n",
    "\n",
    "    #%timeit -r1 -n1 \n",
    "    elbo_record,samps = cs.run(its=250,samples=500)\n",
    "    #TODO: Need to rewrite this method - as modified to handle changing conf_matrices...\n",
    "    allprobs = compute_posterior_probs(cs)\n",
    "    #print_confusion_matrices(allprobs,conf_matrices)\n",
    "    colfil_results = collaborative_filtering_SVD(D,len(priorp),refsensor)\n",
    "    trust = get_trust_weights(D,len(priorp))\n",
    "    #TODO: Need to rewrite this method - as modified to handle changing conf_matrices...\n",
    "    results = evaluate(D,truebees,priorp,allprobs,colfil_results,cs.refsensor,trust)\n",
    "    allresults.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for results in allresults:\n",
    "    printresults(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(allresults,open('allresults_2fold.pkl','wb'))\n",
    "#allresults = pickle.load(open('allresults.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = {}\n",
    "for results in allresults:\n",
    "    for a,b in results.items():\n",
    "        if a not in combined: combined[a] = []\n",
    "        combined[a].extend(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calibration.baseline_categorical_methods import printresults\n",
    "\n",
    "printresults(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output was:\n",
    "```\n",
    "#5fold xval\n",
    "=====PERCENT CORRECT=====\n",
    "Calibration Method            84.2% correct\n",
    "Collaborative Filtering       57.9% correct\n",
    "Baseline (most guessed)       80.0% correct\n",
    "Baseline (\", trust weighted)  82.1% correct\n",
    "Baseline (\", prior weighted)  78.9% correct\n",
    "Baseline (most common)        53.7% correct\n",
    "======NLPD======\n",
    "Calibration Method            59.89\n",
    "Collaborative Filtering       inf\n",
    "Baseline (most guessed)       69.41\n",
    "Baseline (\", trust weighted)  91.74\n",
    "Baseline (\", prior weighted)  98.42\n",
    "Baseline (most common)        130.50\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on all data to get confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare = ['Bombus bohemicus', 'Bombus jonellus', 'Bombus sylvestris', 'Bombus muscorum','Bombus campestris', 'Apis mellifera','Bombus monticola']\n",
    "#rare.extend(['Bombus pratorum','Bombus hypnorum','Bombus hortorum']) #temp to make it quick\n",
    "D, priorp, truebees = synthetic.build_D_from_csv('beelabels.csv',rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\", \".join([\"\\emph{%s}\"% r for r in rare]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head 'beelabels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priorp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(~np.isnan(D[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.any(~np.isnan(D[~np.isnan(D[:,-1]),:-1]),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "a = np.c_[[np.sum(D[:,-1]==i) for i in range(13)],priorp*np.sum(~np.isnan(D))]\n",
    "(a/np.sum(a,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(~np.isnan(D[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rare = ['Bombus bohemicus', 'Bombus jonellus', 'Bombus sylvestris', 'Bombus muscorum','Bombus campestris', 'Apis mellifera','Bombus monticola']\n",
    "#rare.extend(['Bombus pratorum','Bombus hypnorum','Bombus hortorum']) #temp to make it quick\n",
    "D, priorp, truebees = synthetic.build_D_from_csv('beelabels.csv',rare)\n",
    "\n",
    "refsensor = np.zeros(D.shape[1])\n",
    "refsensor[-1] = 1\n",
    "X,Y = synthetic.buildXY_from_D(D)\n",
    "\n",
    "Z = np.linspace(0,1,1)[:,None]\n",
    "ks = [gpflow.kernels.Bias(100)]\n",
    "kernelindices = [[0]*len(refsensor)]*(len(priorp)**2)\n",
    "\n",
    "cs = AltCalibrationSystem(X, Y, Z, refsensor, int(len(priorp)**2), ks, kernelindices,lr=0.04,minibatchsize=10,priorp=priorp)\n",
    "\n",
    "elbo_record,samps = cs.run(its=250,samples=500)\n",
    "#TODO: Need to rewrite this method - as modified to handle changing conf_matrices...\n",
    "allprobs = compute_posterior_probs(cs)\n",
    "print_confusion_matrices(allprobs,None)\n",
    "#colfil_results = collaborative_filtering_SVD(D,len(priorp),refsensor)\n",
    "#trust = get_trust_weights(D,len(priorp))\n",
    "#results = evaluate(D,truebees,priorp,allprobs,colfil_results,cs.refsensor,trust)\n",
    "#allresults.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Record from last run:\n",
    "\n",
    "Person 0\n",
    "--mean posterior--\n",
    "[[70. 11.  8. 22. 11. 21.]\n",
    " [ 2. 17.  8. 10. 14.  5.]\n",
    " [ 3. 20. 55. 14. 20. 12.]\n",
    " [ 3. 11.  8.  7. 10.  8.]\n",
    " [ 5. 19. 14. 29. 27. 17.]\n",
    " [18. 21.  7. 17. 17. 36.]]\n",
    "95% CIs:\n",
    "[[5. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 2. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 0. 0.]] \n",
    "\n",
    " [[ 99. 100.  49. 100.  96. 100.]\n",
    " [ 14.  99.  73.  98.  95.  60.]\n",
    " [ 30. 100.  98. 100.  99.  99.]\n",
    " [ 27.  99.  70.  84.  95.  83.]\n",
    " [ 43. 100.  79. 100. 100.  99.]\n",
    " [ 92. 100.  46. 100. 100. 100.]]\n",
    "-------------------\n",
    "Person 1\n",
    "--mean posterior--\n",
    "[[97.  6.  2.  3. 15. 87.]\n",
    " [ 1. 64.  1.  3. 11.  2.]\n",
    " [ 1.  9. 94.  4. 25.  4.]\n",
    " [ 1.  9.  1. 86.  6.  3.]\n",
    " [ 1.  7.  1.  3. 37.  4.]\n",
    " [ 0.  6.  1.  1.  6.  1.]]\n",
    "95% CIs:\n",
    "[[88.  0.  0.  0.  0. 40.]\n",
    " [ 0.  9.  0.  0.  0.  0.]\n",
    " [ 0.  0. 83.  0.  0.  0.]\n",
    " [ 0.  0.  0. 51.  0.  0.]\n",
    " [ 0.  0.  0.  0.  0.  0.]\n",
    " [ 0.  0.  0.  0.  0.  0.]] \n",
    "\n",
    " [[ 99.  37.  10.  16.  96. 100.]\n",
    " [  6.  99.   6.  32.  96.  21.]\n",
    " [  3.  67.  99.  38.  96.  31.]\n",
    " [  3.  48.   7. 100.  47.  21.]\n",
    " [  4.  70.  13.  26.  99.  44.]\n",
    " [  2.  49.   5.   9.  57.   7.]]\n",
    "-------------------\n",
    "Person 2\n",
    "--mean posterior--\n",
    "[[95. 23. 20. 13. 27. 34.]\n",
    " [ 1. 18.  5.  7. 11. 12.]\n",
    " [ 1. 19. 51. 13. 28. 15.]\n",
    " [ 1. 14.  9. 53. 13. 22.]\n",
    " [ 1. 13.  9.  8. 15. 10.]\n",
    " [ 2. 13.  6.  5.  6.  9.]]\n",
    "95% CIs:\n",
    "[[72.  0.  0.  0.  0.  0.]\n",
    " [ 0.  0.  0.  0.  0.  0.]\n",
    " [ 0.  0.  1.  0.  0.  0.]\n",
    " [ 0.  0.  0.  2.  0.  0.]\n",
    " [ 0.  0.  0.  0.  0.  0.]\n",
    " [ 0.  0.  0.  0.  0.  0.]] \n",
    "\n",
    " [[100. 100.  95.  76. 100. 100.]\n",
    " [  4. 100.  31.  69.  97.  99.]\n",
    " [  6. 100. 100.  83. 100. 100.]\n",
    " [  6.  99.  75.  99.  99. 100.]\n",
    " [  6.  99.  57.  71.  99.  99.]\n",
    " [ 15. 100.  66.  58.  91. 100.]]\n",
    "-------------------\n",
    "Person 3\n",
    "--mean posterior--\n",
    "[[80. 18.  3.  5. 21. 30.]\n",
    " [ 1.  6.  3.  3. 13.  4.]\n",
    " [ 3. 18. 86.  4. 25.  4.]\n",
    " [ 2. 23.  3. 77. 18.  3.]\n",
    " [ 2. 12.  2.  5. 11.  5.]\n",
    " [13. 22.  2.  6. 13. 54.]]\n",
    "95% CIs:\n",
    "[[36.  0.  0.  0.  0.  1.]\n",
    " [ 0.  0.  0.  0.  0.  0.]\n",
    " [ 0.  0. 51.  0.  0.  0.]\n",
    " [ 0.  0.  0.  7.  0.  0.]\n",
    " [ 0.  0.  0.  0.  0.  0.]\n",
    " [ 0.  0.  0.  0.  0.  1.]] \n",
    "\n",
    " [[ 98. 100.  17.  34. 100.  85.]\n",
    " [ 11.  93.  34.  37.  98.  38.]\n",
    " [ 15. 100.  98.  57. 100.  52.]\n",
    " [  8. 100.  33. 100. 100.  18.]\n",
    " [ 13.  99.  14.  47.  98.  48.]\n",
    " [ 57. 100.  29.  52. 100.  98.]]\n",
    "-------------------\n",
    "Person 4\n",
    "--mean posterior--\n",
    "[[88. 40.  4.  5. 16. 40.]\n",
    " [ 1.  5.  1.  4.  6.  4.]\n",
    " [ 4. 36. 87.  7. 31. 11.]\n",
    " [ 4.  6.  6. 74. 28.  8.]\n",
    " [ 1.  6.  1.  5.  9.  8.]\n",
    " [ 2.  7.  2.  5. 10. 29.]]\n",
    "95% CIs:\n",
    "[[52.  1.  0.  0.  0.  0.]\n",
    " [ 0.  0.  0.  0.  0.  0.]\n",
    " [ 0.  1. 33.  0.  0.  0.]\n",
    " [ 0.  0.  0. 11.  0.  0.]\n",
    " [ 0.  0.  0.  0.  0.  0.]\n",
    " [ 0.  0.  0.  0.  0.  0.]] \n",
    "\n",
    " [[ 98.  94.  19.  29. 100.  97.]\n",
    " [  9.  43.   5.  59.  98.  44.]\n",
    " [ 33.  95. 100.  44. 100.  76.]\n",
    " [ 22.  43.  49.  99. 100.  73.]\n",
    " [  7.  61.   4.  59.  80.  80.]\n",
    " [ 15.  66.  25.  32.  98.  99.]]\n",
    "-------------------\n",
    "Person 5\n",
    "--mean posterior--\n",
    "[[76. 71.  2.  4. 16. 55.]\n",
    " [ 3.  9.  1.  8. 12.  1.]\n",
    " [ 1.  4. 78.  8. 14.  2.]\n",
    " [ 1.  4.  1. 60. 12.  1.]\n",
    " [18.  7. 17. 17. 41. 13.]\n",
    " [ 1.  4.  1.  3.  5. 28.]]\n",
    "95% CIs:\n",
    "[[62.  1.  0.  0.  0. 16.]\n",
    " [ 1.  0.  0.  0.  0.  0.]\n",
    " [ 0.  0. 42.  0.  0.  0.]\n",
    " [ 0.  0.  0. 23.  0.  0.]\n",
    " [ 9.  0.  2.  1.  0.  0.]\n",
    " [ 0.  0.  0.  0.  0.  3.]] \n",
    "\n",
    " [[ 87.  99.   8.  27.  99.  88.]\n",
    " [  8.  98.   8.  43.  92.  10.]\n",
    " [  5.  36.  96.  35.  81.   9.]\n",
    " [  2.  32.   7.  95.  76.   6.]\n",
    " [ 34.  70.  56.  60. 100.  67.]\n",
    " [  4.  28.   7.  20.  53.  71.]]\n",
    "-------------------\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
